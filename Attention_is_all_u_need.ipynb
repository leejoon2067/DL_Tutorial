{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995e2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7e6776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I\n",
      "1 am\n",
      "2 a\n",
      "3 graduate\n",
      "4 student\n"
     ]
    }
   ],
   "source": [
    "tokenized = spacy_en.tokenizer('I am a graduate student')\n",
    "\n",
    "for i, token in enumerate(tokenized):\n",
    "    print(i, token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a50c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 최신 torchtext API 사용\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 device: {device}\")\n",
    "\n",
    "# 토크나이저 함수 (기존과 동일)\n",
    "def tokenize_de_new(text):\n",
    "    return ['<sos>'] + tokenize_de(text.lower()) + ['<eos>']\n",
    "\n",
    "def tokenize_en_new(text):\n",
    "    return ['<sos>'] + tokenize_en(text.lower()) + ['<eos>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c972d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e08004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# base_dir = os.path.join('.', '.data', 'multi30k')\n",
    "\n",
    "# for name in os.listdir(base_dir):\n",
    "#     sub_path = os.path.join(base_dir, name)\n",
    "#     # 폴더이고, 폴더명과 같은 파일이 그 안에 있으면\n",
    "#     if os.path.isdir(sub_path):\n",
    "#         file_inside = os.path.join(sub_path, name)\n",
    "#         if os.path.isfile(file_inside):\n",
    "#             # 임시 파일명으로 이동\n",
    "#             temp_name = name + '.tmp'\n",
    "#             temp_path = os.path.join(base_dir, temp_name)\n",
    "#             shutil.move(file_inside, temp_path)\n",
    "#             print(f\"{file_inside} → {temp_path} 임시 이동 완료\")\n",
    "#         # 폴더 삭제\n",
    "#         shutil.rmtree(sub_path)\n",
    "#         print(f\"{sub_path} 폴더 삭제 완료\")\n",
    "#         # 임시 파일명을 원래 이름으로 변경\n",
    "#         if os.path.exists(temp_path):\n",
    "#             final_path = os.path.join(base_dir, name)\n",
    "#             os.rename(temp_path, final_path)\n",
    "#             print(f\"{temp_path} → {final_path} 이름 변경 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e811696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# base_dir = os.path.join('.', '.data', 'multi30k')\n",
    "\n",
    "# # 파일명 변경\n",
    "# old_names = ['test_2016_flickr.de', 'test_2016_flickr.en']\n",
    "# new_names = ['test2016.de', 'test2016.en']\n",
    "\n",
    "# for old_name, new_name in zip(old_names, new_names):\n",
    "#     old_path = os.path.join(base_dir, old_name)\n",
    "#     new_path = os.path.join(base_dir, new_name)\n",
    "#     if os.path.exists(old_path):\n",
    "#         os.rename(old_path, new_path)\n",
    "#         print(f\"{old_name} → {new_name} 변경 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfd8445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터: 29000개\n",
      "검증 데이터: 1014개\n",
      "테스트 데이터: 1000개\n"
     ]
    }
   ],
   "source": [
    "# 직접 파일을 읽어서 데이터셋 생성\n",
    "import os\n",
    "\n",
    "def read_data_files(base_path, split_name):\n",
    "    de_file = os.path.join(base_path, f\"{split_name}.de\")\n",
    "    en_file = os.path.join(base_path, f\"{split_name}.en\")\n",
    "    \n",
    "    with open(de_file, 'r', encoding='utf-8') as f_de, \\\n",
    "         open(en_file, 'r', encoding='utf-8') as f_en:\n",
    "        de_lines = [line.strip() for line in f_de]\n",
    "        en_lines = [line.strip() for line in f_en]\n",
    "    \n",
    "    return list(zip(de_lines, en_lines))\n",
    "\n",
    "# 데이터 로딩\n",
    "data_path = '.data/multi30k'\n",
    "train_data = read_data_files(data_path, 'train')\n",
    "valid_data = read_data_files(data_path, 'val')\n",
    "\n",
    "# test 파일명 처리\n",
    "try:\n",
    "    test_data = read_data_files(data_path, 'test_2016_flickr')\n",
    "except:\n",
    "    test_data = read_data_files(data_path, 'test2016')\n",
    "\n",
    "print(f\"학습 데이터: {len(train_data)}개\")\n",
    "print(f\"검증 데이터: {len(valid_data)}개\") \n",
    "print(f\"테스트 데이터: {len(test_data)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1f8f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30번째 예시:\n",
      "Source (독일어): Ein Mann, der mit einer Tasse Kaffee an einem Urinal steht.\n",
      "Target (영어): A man standing at a urinal with a coffee cup.\n",
      "\n",
      "토큰화된 결과:\n",
      "Source (독일어): ['<sos>', 'ein', 'mann', ',', 'der', 'mit', 'einer', 'tasse', 'kaffee', 'an', 'einem', 'urinal', 'steht', '.', '<eos>']\n",
      "Target (영어): ['<sos>', 'a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 샘플 확인\n",
    "print(\"30번째 예시:\")\n",
    "print(\"Source (독일어):\", train_data[30][0])\n",
    "print(\"Target (영어):\", train_data[30][1])\n",
    "\n",
    "# 토큰화된 결과 확인\n",
    "print(\"\\n토큰화된 결과:\")\n",
    "print(\"Source (독일어):\", tokenize_de_new(train_data[30][0]))\n",
    "print(\"Target (영어):\", tokenize_en_new(train_data[30][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbcd7960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1lines [00:00, ?lines/s]\n",
      "1lines [00:00, ?lines/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "독일어 어휘집 크기: 7853\n",
      "영어 어휘집 크기: 5893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 어휘집 구축을 위한 토큰 생성기\n",
    "def yield_tokens_de(data_iter):\n",
    "    for de_text, en_text in data_iter:\n",
    "        yield tokenize_de_new(de_text)\n",
    "\n",
    "def yield_tokens_en(data_iter):\n",
    "    for de_text, en_text in data_iter:\n",
    "        yield tokenize_en_new(en_text)\n",
    "\n",
    "# 어휘집 구축 (최신 API)\n",
    "from collections import Counter\n",
    "\n",
    "# 토큰 빈도 계산\n",
    "def get_vocab_with_min_freq(token_generator, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for tokens in token_generator:\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # min_freq 이상인 토큰만 선택\n",
    "    filtered_counter = {token: count for token, count in counter.items() if count >= min_freq}\n",
    "    return filtered_counter\n",
    "\n",
    "# 독일어 어휘집\n",
    "de_counter = get_vocab_with_min_freq(yield_tokens_de(train_data), min_freq=2)\n",
    "\n",
    "# 특수 토큰을 어휘에 먼저 추가\n",
    "special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "de_vocab_tokens = special_tokens + list(de_counter.keys())\n",
    "\n",
    "SRC_vocab = build_vocab_from_iterator([de_vocab_tokens])\n",
    "\n",
    "# 영어 어휘집  \n",
    "en_counter = get_vocab_with_min_freq(yield_tokens_en(train_data), min_freq=2)\n",
    "en_vocab_tokens = special_tokens + list(en_counter.keys())\n",
    "\n",
    "TRG_vocab = build_vocab_from_iterator([en_vocab_tokens])\n",
    "\n",
    "# 기본 토큰 설정 (최신 API)\n",
    "# UNK 토큰을 기본값으로 설정 (인덱스 0)\n",
    "unk_idx = 0  # 특수 토큰 리스트에서 <unk>가 첫 번째\n",
    "\n",
    "# Vocab 객체를 더 유연하게 만들기 위해 래퍼 함수 정의\n",
    "def create_vocab_lookup(vocab, unk_idx=0):\n",
    "    \"\"\"어휘집에서 토큰을 인덱스로 변환하는 함수\"\"\"\n",
    "    def lookup_tokens(tokens):\n",
    "        if isinstance(tokens, str):\n",
    "            return vocab.get(tokens, unk_idx)\n",
    "        return [vocab.get(token, unk_idx) for token in tokens]\n",
    "    return lookup_tokens\n",
    "\n",
    "# 어휘집 검색 함수 생성 (최신 API 호환)\n",
    "# Vocab 객체에서 직접 토큰-인덱스 매핑 생성\n",
    "try:\n",
    "    # 최신 버전 시도\n",
    "    src_tokens = list(SRC_vocab.get_itos())\n",
    "    trg_tokens = list(TRG_vocab.get_itos())\n",
    "except AttributeError:\n",
    "    # 다른 방법으로 토큰 리스트 얻기\n",
    "    src_tokens = de_vocab_tokens\n",
    "    trg_tokens = en_vocab_tokens\n",
    "\n",
    "SRC_lookup = create_vocab_lookup(dict(zip(src_tokens, range(len(src_tokens)))))\n",
    "TRG_lookup = create_vocab_lookup(dict(zip(trg_tokens, range(len(trg_tokens)))))\n",
    "\n",
    "print(f\"독일어 어휘집 크기: {len(SRC_vocab)}\")\n",
    "print(f\"영어 어휘집 크기: {len(TRG_vocab)}\")\n",
    "\n",
    "# 기존 코드와의 호환성을 위한 클래스 생성\n",
    "class VocabWrapper:\n",
    "    def __init__(self, vocab, lookup_func, tokens):\n",
    "        self.vocab = vocab\n",
    "        self.stoi = {token: idx for idx, token in enumerate(tokens)}\n",
    "        self.itos = tokens\n",
    "        self.pad_token = '<pad>'\n",
    "        self.lookup = lookup_func\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        \"\"\"vocab['token'] 형태로 사용 가능\"\"\"\n",
    "        return self.stoi.get(token, 0)  # 기본값은 <unk> 인덱스 0\n",
    "\n",
    "# 기존 변수명 유지\n",
    "SRC = VocabWrapper(SRC_vocab, SRC_lookup, src_tokens)\n",
    "TRG = VocabWrapper(TRG_vocab, TRG_lookup, trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80502f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘집 테스트:\n",
      "abcabc (없는 단어): 0\n",
      "<pad> 토큰: 1\n",
      "<sos> 토큰: 4\n",
      "hello: 4529\n",
      "\n",
      "GPU 사용 가능: True\n",
      "GPU 이름: NVIDIA GeForce RTX 4070 Ti\n",
      "테스트 텐서가 cuda:0에 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"어휘집 테스트:\")\n",
    "print(f\"abcabc (없는 단어): {TRG.stoi.get('abcabc', TRG.stoi['<unk>'])}\")\n",
    "print(f\"<pad> 토큰: {TRG.stoi['<pad>']}\")\n",
    "print(f\"<sos> 토큰: {TRG.stoi['<sos>']}\")\n",
    "print(f\"hello: {TRG.stoi.get('hello', TRG.stoi['<unk>'])}\")\n",
    "\n",
    "# GPU 확인\n",
    "print(f\"\\nGPU 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # GPU 테스트\n",
    "    test_tensor = torch.randn(3, 3).to(device)\n",
    "    print(f\"테스트 텐서가 {test_tensor.device}에 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c90d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.7.1+cu118\n",
      "CUDA 사용 가능: True\n",
      "CUDA 버전: 11.8\n",
      "cuDNN 버전: 90100\n",
      "GPU 개수: 1\n",
      "GPU 이름: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "print(f\"cuDNN 버전: {torch.backends.cudnn.version()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA를 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981d7c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 device: cuda\n",
      "DataLoader 생성 완료!\n",
      "학습 배치 수: 227\n",
      "검증 배치 수: 8\n",
      "테스트 배치 수: 8\n",
      "\n",
      "샘플 배치 크기:\n",
      "Source: torch.Size([128, 30])\n",
      "Target: torch.Size([128, 29])\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 최신 PyTorch DataLoader로 BucketIterator 대체\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 device: {device}\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 커스텀 Dataset 클래스\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_vocab, trg_vocab, tokenize_src, tokenize_trg):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.tokenize_src = tokenize_src\n",
    "        self.tokenize_trg = tokenize_trg\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text, trg_text = self.data[idx]\n",
    "        \n",
    "        # 토큰화\n",
    "        src_tokens = self.tokenize_src(src_text)\n",
    "        trg_tokens = self.tokenize_trg(trg_text)\n",
    "        \n",
    "        # 인덱스로 변환\n",
    "        src_indices = [self.src_vocab[token] for token in src_tokens]\n",
    "        trg_indices = [self.trg_vocab[token] for token in trg_tokens]\n",
    "        \n",
    "        return torch.tensor(src_indices), torch.tensor(trg_indices)\n",
    "\n",
    "# collate 함수 (배치 처리용)\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    \n",
    "    # 패딩\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=SRC['<pad>'])\n",
    "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=TRG['<pad>'])\n",
    "    \n",
    "    return src_batch, trg_batch\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset_new = TranslationDataset(train_data, SRC, TRG, tokenize_de_new, tokenize_en_new)\n",
    "valid_dataset_new = TranslationDataset(valid_data, SRC, TRG, tokenize_de_new, tokenize_en_new)\n",
    "test_dataset_new = TranslationDataset(test_data, SRC, TRG, tokenize_de_new, tokenize_en_new)\n",
    "\n",
    "# DataLoader 생성 (BucketIterator 대체)\n",
    "train_iterator = DataLoader(\n",
    "    train_dataset_new, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "valid_iterator = DataLoader(\n",
    "    valid_dataset_new, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_iterator = DataLoader(\n",
    "    test_dataset_new, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"DataLoader 생성 완료!\")\n",
    "print(f\"학습 배치 수: {len(train_iterator)}\")\n",
    "print(f\"검증 배치 수: {len(valid_iterator)}\")\n",
    "print(f\"테스트 배치 수: {len(test_iterator)}\")\n",
    "\n",
    "# 샘플 배치 확인\n",
    "sample_batch = next(iter(train_iterator))\n",
    "src_sample, trg_sample = sample_batch\n",
    "print(f\"\\n샘플 배치 크기:\")\n",
    "print(f\"Source: {src_sample.shape}\")\n",
    "print(f\"Target: {trg_sample.shape}\")\n",
    "print(f\"Device: {src_sample.device if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "023ee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치 크기 - Source: torch.Size([128, 24]), Target: torch.Size([128, 27])\n",
      "Device: cuda:0\n",
      "\n",
      "첫 번째 문장의 토큰 인덱스:\n",
      "인덱스 0: 4 -> '<sos>'\n",
      "인덱스 1: 23 -> 'ein'\n",
      "인덱스 2: 541 -> 'bauarbeiter'\n",
      "인덱스 3: 34 -> 'steht'\n",
      "인덱스 4: 92 -> 'oben'\n",
      "인덱스 5: 35 -> 'auf'\n",
      "인덱스 6: 36 -> 'einer'\n",
      "인덱스 7: 4154 -> 'holzkonstruktion'\n",
      "인덱스 8: 17 -> '.'\n",
      "인덱스 9: 18 -> '<eos>'\n"
     ]
    }
   ],
   "source": [
    "# 새로운 DataLoader 형태로 배치 처리\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    src, trg = batch  # 튜플 언패킹\n",
    "    \n",
    "    # GPU로 데이터 이동\n",
    "    src = src.to(device)\n",
    "    trg = trg.to(device)\n",
    "\n",
    "    print(f\"첫 번째 배치 크기 - Source: {src.shape}, Target: {trg.shape}\")\n",
    "    print(f\"Device: {src.device}\")\n",
    "\n",
    "    print(\"\\n첫 번째 문장의 토큰 인덱스:\")\n",
    "    for j in range(min(10, src.shape[1])):  # 처음 10개 토큰만 출력\n",
    "        token_idx = src[0][j].item()\n",
    "        token = SRC.itos[token_idx] if token_idx < len(SRC.itos) else '<unk>'\n",
    "        print(f\"인덱스 {j}: {token_idx} -> '{token}'\")\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c622d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c9b819",
   "metadata": {},
   "source": [
    "## 1. Multi-head attention 아키텍쳐\n",
    "\n",
    "- 세 가지 요소를 입력을 받는 attention\n",
    "    - 쿼리 (Queries)\n",
    "    - 키 (Keys)\n",
    "    - 값 (Values)\n",
    "- 하이퍼 파라미터\n",
    "    - hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "    - n_heads : 헤드의 개수 = scaled dot-product attention의 개수\n",
    "    - dropout_ratio : 드롭아웃 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3571dbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 긴급 디버깅: CPU 모드 전환 ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 모든 것을 CPU로 이동\u001b[39;00m\n\u001b[0;32m      5\u001b[0m device_cpu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model_cpu \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 샘플 배치로 테스트\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_iterator))\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1336\u001b[0m             device,\n\u001b[0;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1338\u001b[0m             non_blocking,\n\u001b[0;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1340\u001b[0m         )\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 긴급 해결: CPU 모드로 전환하여 디버깅\n",
    "print(\"=== 긴급 디버깅: CPU 모드 전환 ===\")\n",
    "\n",
    "# 모든 것을 CPU로 이동\n",
    "device_cpu = torch.device('cpu')\n",
    "model_cpu = model.to(device_cpu)\n",
    "\n",
    "# 샘플 배치로 테스트\n",
    "sample_batch = next(iter(train_iterator))\n",
    "src_sample, trg_sample = sample_batch\n",
    "\n",
    "# CPU로 이동\n",
    "src_cpu = src_sample.to(device_cpu)\n",
    "trg_cpu = trg_sample.to(device_cpu)\n",
    "\n",
    "print(f\"Source shape: {src_cpu.shape}\")\n",
    "print(f\"Target shape: {trg_cpu.shape}\")\n",
    "\n",
    "# 인덱스 범위 확인\n",
    "print(f\"\\nSource 인덱스 범위: {src_cpu.min().item()} ~ {src_cpu.max().item()}\")\n",
    "print(f\"Target 인덱스 범위: {trg_cpu.min().item()} ~ {trg_cpu.max().item()}\")\n",
    "print(f\"Source vocab 크기: {len(SRC)}\")\n",
    "print(f\"Target vocab 크기: {len(TRG)}\")\n",
    "\n",
    "# 범위 초과 체크\n",
    "if src_cpu.max().item() >= len(SRC):\n",
    "    print(f\"⚠️ SOURCE 범위 초과! 최대 인덱스: {src_cpu.max().item()}, vocab 크기: {len(SRC)}\")\n",
    "    \n",
    "if trg_cpu.max().item() >= len(TRG):\n",
    "    print(f\"⚠️ TARGET 범위 초과! 최대 인덱스: {trg_cpu.max().item()}, vocab 크기: {len(TRG)}\")\n",
    "\n",
    "# CPU에서 모델 테스트\n",
    "try:\n",
    "    print(\"\\nCPU에서 모델 테스트...\")\n",
    "    with torch.no_grad():\n",
    "        output, _ = model_cpu(src_cpu[:1], trg_cpu[:1, :-1])  # 첫 번째 샘플만\n",
    "    print(\"✅ CPU에서 모델 실행 성공!\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ CPU에서도 오류 발생: {e}\")\n",
    "\n",
    "# 특수 토큰 인덱스 확인\n",
    "print(f\"\\n=== 특수 토큰 인덱스 ===\")\n",
    "print(f\"SRC <pad>: {SRC.stoi['<pad>']}\")\n",
    "print(f\"SRC <sos>: {SRC.stoi['<sos>']}\")\n",
    "print(f\"SRC <eos>: {SRC.stoi['<eos>']}\")\n",
    "print(f\"TRG <pad>: {TRG.stoi['<pad>']}\")\n",
    "print(f\"TRG <sos>: {TRG.stoi['<sos>']}\")\n",
    "print(f\"TRG <eos>: {TRG.stoi['<eos>']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "989cef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 디버깅 train 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# CPU 모드로 수정된 train 함수\n",
    "def train_cpu_debug(model, iterator, optimizer, criterion, clip, device_override=None):\n",
    "    \"\"\"디버깅용 train 함수 - CPU 모드\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    device_to_use = device_override if device_override else torch.device('cpu')\n",
    "    \n",
    "    print(f\"학습 시작 - Device: {device_to_use}\")\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        # 배치 언패킹\n",
    "        src, trg = batch\n",
    "        \n",
    "        # 디바이스로 이동\n",
    "        src = src.to(device_to_use)\n",
    "        trg = trg.to(device_to_use)\n",
    "        \n",
    "        # 인덱스 범위 체크\n",
    "        if src.max().item() >= len(SRC):\n",
    "            print(f\"배치 {i}: SRC 인덱스 초과 {src.max().item()} >= {len(SRC)}\")\n",
    "            continue\n",
    "        if trg.max().item() >= len(TRG):\n",
    "            print(f\"배치 {i}: TRG 인덱스 초과 {trg.max().item()} >= {len(TRG)}\")\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            # 모델 실행\n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            # 출력 변환\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg_flat = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg_flat)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"배치 {i}/{len(iterator)}: Loss = {loss.item():.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"배치 {i}에서 오류: {e}\")\n",
    "            print(f\"SRC shape: {src.shape}, TRG shape: {trg.shape}\")\n",
    "            print(f\"SRC range: {src.min()}-{src.max()}, TRG range: {trg.min()}-{trg.max()}\")\n",
    "            break\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "print(\"CPU 디버깅 train 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58b953eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPU 모드 테스트 실행 ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_cpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== CPU 모드 테스트 실행 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 옵티마이저를 CPU 모델용으로 재생성\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m optimizer_cpu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel_cpu\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# CPU 모드에서 한 배치만 테스트\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_cpu' is not defined"
     ]
    }
   ],
   "source": [
    "# CPU 모드에서 테스트 실행\n",
    "print(\"=== CPU 모드 테스트 실행 ===\")\n",
    "\n",
    "# 옵티마이저를 CPU 모델용으로 재생성\n",
    "optimizer_cpu = torch.optim.Adam(model_cpu.parameters(), lr=0.0005)\n",
    "\n",
    "# CPU 모드에서 한 배치만 테스트\n",
    "try:\n",
    "    print(\"CPU 모드에서 한 배치 테스트...\")\n",
    "    test_loss = train_cpu_debug(model_cpu, train_iterator, optimizer_cpu, criterion, 1, torch.device('cpu'))\n",
    "    print(f\"✅ CPU 테스트 성공! Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # 문제가 없다면 GPU로 다시 시도\n",
    "    print(\"\\n=== GPU 모드 재시도 준비 ===\")\n",
    "    print(\"문제가 해결되었다면 다음 단계:\")\n",
    "    print(\"1. 모델을 GPU로 다시 이동\")\n",
    "    print(\"2. 옵티마이저 재생성\")\n",
    "    print(\"3. GPU에서 학습 시작\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ CPU에서도 오류: {e}\")\n",
    "    print(\"근본적인 데이터 문제가 있습니다.\")\n",
    "    \n",
    "    # 상세 디버깅\n",
    "    print(\"\\n=== 상세 디버깅 ===\")\n",
    "    sample_src, sample_trg = next(iter(train_iterator))\n",
    "    print(f\"샘플 데이터:\")\n",
    "    print(f\"  SRC: {sample_src[0][:5]}\")  # 첫 5개 토큰만\n",
    "    print(f\"  TRG: {sample_trg[0][:5]}\")\n",
    "    \n",
    "    # 실제 토큰 확인\n",
    "    print(f\"\\n실제 토큰:\")\n",
    "    for i, idx in enumerate(sample_src[0][:5]):\n",
    "        if idx.item() < len(SRC.itos):\n",
    "            token = SRC.itos[idx.item()]\n",
    "            print(f\"  SRC[{i}]: {idx.item()} -> '{token}'\")\n",
    "        else:\n",
    "            print(f\"  SRC[{i}]: {idx.item()} -> INVALID!\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a6f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % n_heads == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim # 임베딩 차원\n",
    "        self.n_heads = n_heads # 헤드(head)의 개수, 서로 다른 attention 컨셉 수\n",
    "        self.head_dim = hidden_dim // n_heads # 각 헤드의 차원\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
    "\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim) # 최종 결과에 적용될 FC 레이어\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)  \n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # query : [batch_size, query_len, hidden_dim]\n",
    "        # key : [batch_size, key_len, hidden_dim]\n",
    "        # value : [batch_size, value_len, hidden_dim]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # hidden_dim => n_heads x head_dim 형태로 변형 \n",
    "        # view: 텐서의 shape을 바꿔줌. -1은 자동 계산.\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        # Q : [batch_size, n_heads, query_len, head_dim]\n",
    "        # K : [batch_size, n_heads, key_len, head_dim]\n",
    "        # V : [batch_size, n_heads, value_len, head_dim]\n",
    "\n",
    "        # Attention Energy 계산\n",
    "        energy = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale\n",
    "\n",
    "        # 마스크 (mask)를 사용하는 경우\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask ==0, -1e10)\n",
    "\n",
    "        # attention 스코어에 대한 softmax 연산\n",
    "        # attention : [batch)size, n_heads, query_len, key_len]\n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "\n",
    "        # Scaled Dot-Product Attention을 계산\n",
    "        # x : [batch_size, n_heads, query_len, head_dim]\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # permute는 텐서 차원 순서 변경, contiguous는 메모리 연속성 보장 함수\n",
    "        # x : [batch_size, query_len, n_heads, head_dim]\n",
    "        x = x.permute(0,2,1,3).contiguous()\n",
    "        \n",
    "        # x : [batch_size, query_len, hidden_dim]\n",
    "        x = x.view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9cf73c",
   "metadata": {},
   "source": [
    "## 2. Position-wise Feedforward 아키텍쳐\n",
    "- 입력과 출력이 동일\n",
    "- 하이퍼 파라미터\n",
    "    - hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "    - pf_dim : feedforward 레이어에서의 내부 임베딩 차원\n",
    "    - dropout_ratio : 드롭아웃 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d75100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e3488",
   "metadata": {},
   "source": [
    "## 3-1. Encoder 레이어 아키텍쳐\n",
    "- 하나의 인코더 레이어에 대해 정의\n",
    "    - 입력과 출력의 차원이 같다.\n",
    "    - 이 특징을 사용해 트랜스포머의 인코더는 인코더 레이어를 여러 번 중첩해서 사용함.\n",
    "\n",
    "- 하이퍼 파라미터\n",
    "    - hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "    - n_heads : 헤드의 개수 = scaled dot-product attention의 개수\n",
    "    - dropout_ratio : 드롭아웃 비율\n",
    "\n",
    "- <pad> 토큰에 대해 마스크 값을 0으로 설정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e1a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attn = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 하나의 임베딩이 복제되어 Q, K, V 로 입력되는 방식\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "\n",
    "        # src : [batch_size, src_len, hidden_dim]\n",
    "        # src_mask : [batch_size, src_len]\n",
    "\n",
    "        # self attention\n",
    "        # 필요 시 마스크 행렬을 이용해 어텐션할 단어 조절 가능\n",
    "\n",
    "        # self.self_attn은 MultiHeadAttentionLayer를 의미하며, 이 레이어는 입력값으로 Q(Query), K(Key), V(Value), 그리고 마스크(src_mask)를 받습니다.\n",
    "        # 트랜스포머 인코더에서는 self-attention이므로 Q, K, V 모두 동일한 입력(src)을 사용합니다.\n",
    "\n",
    "        # src_mask는 패딩 토큰 등에 대한 마스킹을 위해 사용됩니다.\n",
    "        # self.self_attn의 반환값은 (attention_output, attention_weights)로, 여기서는 attention_output만 사용하고 attention_weights는 사용하지 않으므로 _로 처리합니다.\n",
    "        \n",
    "        # _src는 self-attention 레이어를 통과한 결과(즉, 어텐션을 적용한 후의 임베딩)이고,\n",
    "        # src는 아직 self-attention을 통과하지 않은 입력 임베딩입니다.\n",
    "        \n",
    "        # self-attention의 출력(_src)에 드롭아웃을 적용한 뒤, 입력(src)와 더해주고,\n",
    "        # 그 결과를 LayerNorm에 통과시켜서 최종적으로 src를 업데이트합니다.\n",
    "\n",
    "        _src, _ = self.self_attn(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        return src\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4700cc",
   "metadata": {},
   "source": [
    "## 3-2. Encoder 아키텍쳐\n",
    "- 전체 인코터 아키텍쳐의 정의\n",
    "- 하이퍼 파라미터\n",
    "    - input_dim : 하나의 단어에 대한 원-핫 인코딩 차원\n",
    "    - hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "    - n_layers : 내부적으로 사용할 인코더 레이어의 개수\n",
    "    - n_heads: 헤드의 개수 = scaled dot-product attention의 개수\n",
    "    - pf_dim : feedforward 레이어에서의 내부 임베딩 차원\n",
    "    - dropout_ratio : 드롭아웃 비율\n",
    "    - max_length : 문장 내 최대 단어 개수\n",
    "- 위치 임베딩 (positional embedding)을 학습하는 형태로 구현\n",
    "- <pad> 토큰에 대해 마스크 값을 0으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d41114ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404fd95e",
   "metadata": {},
   "source": [
    "## 4-1. Decoder 레이어 아키텍쳐\n",
    "- 하나의 디코더 레이어에 대해 정의\n",
    "    - 입력과 출력의 차원이 같음.\n",
    "    - 이 특징을 이용해 인코더와 마찬가지로 디코더 레이어를 여러 번 중첩해서 사용.\n",
    "    - 디코더 레이어에서는 두 개의 multi-head attention 레이어를 사용.\n",
    "- 하이퍼 파라미터\n",
    "    - hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "    - n_heads : 헤드의 개수 = scaled dot-product attention의 개수\n",
    "    - pf_dim : feedforward 레이어에서의 내부 임베딩 차원\n",
    "    - dropout_ratio : 드롭아웃 비율\n",
    "- 소스 문장의 <pad> 토큰에 대해 마스크 값을 0으로 설정\n",
    "- 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없게 (바로 이전 단어만 보도록) 마스크를 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e7547",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "098ff2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.self_attn = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.encoder_attn = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더의 출력값(enc_src)을 attention 과정에서 사용하기 위해 입력값으로 받음\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # 1. self attention -> 자기 자신에 대해 어텐션 적용\n",
    "        _trg, _ = self.self_attn(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # dropout, residual connection & layer normalization\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # 2. encoder attention -> 디코더의 쿼리를 이용해 인코더를 어텐션\n",
    "        _trg, attention = self.encoder_attn(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection & layer normalization\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # 3. positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual connection & layer normalization\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49487dab",
   "metadata": {},
   "source": [
    "## 4-2. Decoder 아키텍쳐\n",
    "- 전체 디코더 아키텍쳐를 정의\n",
    "- 하이퍼 파라미터\n",
    "    - output_dim : 하나의 단어에 대한 원-핫 인코딩 차원\n",
    "    - hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "    - n_layers : 내부적으로 사용할 인코더 레이어의 개수\n",
    "    - n_heads : 헤드의 개수 = scaled dot-product attention의 개수\n",
    "    - pf_dim : feedforward 레이어에서의 내부 임베딩 차원\n",
    "    - dropout_ratio : 드롭아웃 비율\n",
    "    - max_length : 문장 내 최대 단어 개수\n",
    "- 위치 임베딩 (positional embedding)을 학습하는 형태로 구현\n",
    "- Seq2Seq 과 마찬가지로 실제 추론 (inference) 과정에서는 디코더를 반복적으로 넣을 필요가 있음.\n",
    "    - 학습 시기에는 한 번에 출력 문장을 구해 학습함. \n",
    "- 소스 문장의 <pad> 토큰에 대해 마스크 값을 0으로 설정.\n",
    "- 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록 만들기 위해 마스크를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d1e2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers: \n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        return output, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd55e9",
   "metadata": {},
   "source": [
    "## 5. 최종 Transformer 아키텍쳐\n",
    "- 전체 Transformer 모델을 정의함.\n",
    "- 입력을 들어왔을 때 앞서 정의한 인코더와 디코더를 거쳐 출력 문장을 생성함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae49b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # unsqueeze는 텐서의 차원을 늘려주는 함수.\n",
    "        # 예를 들어, (batch_size, src_len) 형태의 src에 대해 unsqueeze(1)을 하면 (batch_size, 1, src_len)이 되고,\n",
    "        # 다시 unsqueeze(2)를 하면 (batch_size, 1, 1, src_len)이 돼.\n",
    "        # 이렇게 차원을 늘려주는 이유는 이후에 어텐션 연산에서 브로드캐스팅이 잘 되도록 맞춰주기 위해서임.\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len, device = self.device)).bool()\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ca9b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 차원 (독일어 어휘 크기): 7853\n",
      "출력 차원 (영어 어휘 크기): 5893\n"
     ]
    }
   ],
   "source": [
    "# 모델 하이퍼파라미터 설정\n",
    "INPUT_DIM = len(SRC)  # 새로운 vocab 구조에 맞게 수정\n",
    "OUTPUT_DIM = len(TRG)  # 새로운 vocab 구조에 맞게 수정\n",
    "\n",
    "print(f\"입력 차원 (독일어 어휘 크기): {INPUT_DIM}\")\n",
    "print(f\"출력 차원 (영어 어휘 크기): {OUTPUT_DIM}\")\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89e1cd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 패딩 인덱스: 1\n",
      "Target 패딩 인덱스: 1\n"
     ]
    }
   ],
   "source": [
    "# 패딩 인덱스 설정 (새로운 vocab 구조에 맞게)\n",
    "SRC_PAD_IDX = SRC.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.stoi[TRG.pad_token]\n",
    "\n",
    "print(f\"Source 패딩 인덱스: {SRC_PAD_IDX}\")\n",
    "print(f\"Target 패딩 인덱스: {TRG_PAD_IDX}\")\n",
    "\n",
    "# 인코더 & 디코더 & Transformer 객체 선언\n",
    "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c5a43",
   "metadata": {},
   "source": [
    "- model 가중치 파라미터 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dddd303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,038,341 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3688bac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(7853, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(5893, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attn): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 함수는 모델의 가중치(weight) 파라미터를 xavier_uniform 방식으로 초기화해주는 함수야.\n",
    "# 신경망의 각 레이어(모듈) m에 대해, 만약 'weight' 속성이 있고 그 차원이 1보다 크면(즉, 선형 계층 등),\n",
    "# nn.init.xavier_uniform을 사용해서 가중치를 초기화해. 이렇게 하면 학습 초기에 적절한 분포로 가중치가 설정되어\n",
    "# 학습이 더 잘 되도록 도와줘.\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)  # xavier_uniform_로 변경 (최신 권장 방식)\n",
    "\n",
    "# 모델의 모든 서브모듈에 대해 위의 초기화 함수를 적용해.\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee705c70",
   "metadata": {},
   "source": [
    "## 6. 학습 및 평가 함수 정의\n",
    "- 모델 학습과 optimization 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e98a647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    device = device_override if device_override else torch.device('cpu')\n",
    "\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # 새로운 DataLoader 형태: 튜플 언패킹\n",
    "        src, trg = batch\n",
    "        \n",
    "        # GPU로 데이터 이동\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 디코더 입력에서 마지막 토큰 제거 (teacher forcing)\n",
    "        output, _ = model(src, trg[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # 출력을 1차원으로 변환\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "\n",
    "        # 타겟에서 첫 번째 토큰(<sos>) 제거하고 1차원으로 변환\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bab17c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            # 새로운 DataLoader 형태: 튜플 언패킹\n",
    "            src, trg = batch\n",
    "            \n",
    "            # GPU로 데이터 이동\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            # 디코더 입력에서 마지막 토큰 제거\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            # 출력을 1차원으로 변환\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "\n",
    "            # 타겟에서 첫 번째 토큰(<sos>) 제거하고 1차원으로 변환\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8af7ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bf75536",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m     10\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# 시작 시간 기록\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_iterator, criterion)\n\u001b[0;32m     15\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# 종료 시간 기록\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 디코더 입력에서 마지막 토큰 제거 (teacher forcing)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 출력을 1차원으로 변환\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[21], line 37\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     34\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_src_mask(src)\n\u001b[0;32m     35\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_trg_mask(trg)\n\u001b[1;32m---> 37\u001b[0m enc_src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m output, attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(trg, enc_src, trg_mask, src_mask)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, attention\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\AimsLab\\Desktop\\Tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     17\u001b[0m src_len \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, src_len)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 20\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(pos))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     23\u001b[0m     src \u001b[38;5;241m=\u001b[39m layer(src, src_mask)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time() # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed9fed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0380523b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05fbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb54af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf61c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
